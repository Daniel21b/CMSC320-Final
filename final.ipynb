{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <div style=\"font-size: 28px;\"><strong>Stock Market Analysis and Prediction Project</strong></div>\n",
    "    <div style=\"font-size: 15px;\">Author: Daniel A, Daniel B, David</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Bullimage.jpg\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"font-size: 1.6em\">Table of Contents</span>** \n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Overview](#overview)\n",
    "\n",
    "4. [Data Curation](#data-curation)\n",
    "   - [Datasets](#datasets)\n",
    "   - [Why We Are Choosing These Datasets](#why-we-are-choosing-these-datasets)\n",
    "\n",
    "5. [Exploratory Data Analysis](#exploratory-data-analysis)\n",
    "   - [Data Preprocessing](#data-preprocessing)\n",
    "   - [Importing and Parsing Data](#importing-and-parsing-data)\n",
    "   - [Organizing Data](#organizing-data)\n",
    "   - [Basic Data Exploration and Summary Statistics](#basic-data-exploration-and-summary-statistics)\n",
    "   - [Main Characteristics](#main-characteristics)\n",
    "   - [Identifying Key Attributes](#identifying-key-attributes)\n",
    "\n",
    "6. [Hypothesis Testing and Statistical Analysis](#hypothesis-testing-and-statistical-analysis)\n",
    "   - [Chi-Squared Test](#chi-squared-test)\n",
    "   - [Z Test](#z-test)\n",
    "   - [T Test](#t-test)\n",
    "   - [Mann-Whitney U Test](#mann-whitney-u-test)\n",
    "   - [ANOVA](#anova)\n",
    "   - [Presenting Conclusions and Visualizations](#presenting-conclusions-and-visualizations)\n",
    "     - [Conclusion 1: Correlation Analysis](#conclusion-1-correlation-analysis)\n",
    "     - [Conclusion 2: Outlier Detection](#conclusion-2-outlier-detection)\n",
    "     - [Conclusion 3: Hypothesis Testing](#conclusion-3-hypothesis-testing)\n",
    "\n",
    "7. [Predictive Modeling](#predictive-modeling)\n",
    "   - [Model Selection](#model-selection)\n",
    "   - [Model Implementation](#model-implementation)\n",
    "   - [Model Evaluation](#model-evaluation)\n",
    "\n",
    "8. [Visualization](#visualization)\n",
    "   - [Explanation of Results](#explanation-of-results)\n",
    "   - [Plots and Graphs](#plots-and-graphs)\n",
    "\n",
    "9. [Insights and Conclusions](#insights-and-conclusions)\n",
    "   - [Summary of Findings](#summary-of-findings)\n",
    "   - [Implications and Recommendations](#implications-and-recommendations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <span style=\"font-size: 1.6em; font-weight: bold\">Data Collection</span>\n",
    "</div>\n",
    "\n",
    "\n",
    "The stock market is a dynamic and complex system that plays a crucial role in the global economy. Understanding stock market trends and predicting future stock prices are essential for investors, policymakers, and financial analysts. This project aims to analyze historical stock market data, identify significant trends, and develop predictive models to forecast future stock prices.\n",
    "\n",
    "By examining data from various sectors, including major financial services and banks, top tech companies, and key stock market indices, we aim to gain a deeper understanding of how different factors influence stock prices. Our analysis will cover periods of economic stability as well as major financial events, such as the 2008 financial crisis and the COVID-19 pandemic. This comprehensive analysis will help us uncover patterns, correlations, and anomalies that can inform investment strategies and economic policies.\n",
    "\n",
    "The main objectives of this project are:\n",
    "\n",
    "    *****Behavioral Analysis:*****   Track and analyze stock price behavior over time to identify trends and patterns.\n",
    "    *****Event Impact Analysis:***** Examine the effects of significant events on stock prices, including financial crises, political changes, and economic policies.\n",
    "    *****Predictive Modeling: *****  Develop models to predict future stock prices using historical data and advanced machine learning techniques.\n",
    "\n",
    "***Data Curation***\n",
    "FSCs-Banks (2006-2020):\n",
    "\n",
    "    Source: Kaggle\n",
    "    URL: https://www.kaggle.com/datasets/dgawlik/nyse\n",
    "    Contains stock information (high, low, close, etc.) for major financial services and banks from January 1, 2006, to November 1, 2020.\n",
    "\n",
    "Stock Market Indices (2006-2020):\n",
    "\n",
    "    Source: Kaggle\n",
    "    URL: https://www.kaggle.com/datasets/borismarjanovic/price-volume-data-for-all-us-stocks-etfs\n",
    "    Provides data on the top five stock market indices from 2006 to 2020.\n",
    "\n",
    "Stock Market Indices (2014-2024):\n",
    "\n",
    "    Source: Kaggle\n",
    "    URL: https://www.kaggle.com/datasets/jacksoncrow/stock-market-dataset\n",
    "    Extends the coverage of stock market indices data from 2014 to 2024.\n",
    "\n",
    "Tech Companies (2006-2020):\n",
    "\n",
    "    Source: Kaggle\n",
    "    URL: https://www.kaggle.com/datasets/szrlee/stock-time-series-20050101-to-20171231\n",
    "    Includes stock information for the top tech companies from January 1, 2006, to November 1, 2020.\n",
    "\n",
    "\n",
    "***Why We Are Choosing These Datasets***\n",
    "****Historical Coverage:**** These datasets provide extensive historical data from 2006 to 2024, covering key periods such as the 2008 financial crisis and the COVID-19 pandemic.\n",
    "****Diverse Sectors:**** By including financial services, banks, tech companies, and major market indices, we can perform a comprehensive analysis across different sectors.\n",
    "****Event Impact Analysis:**** These datasets allow us to examine the effects of significant events (e.g., financial crises, political changes) on stock prices, offering valuable insights into market behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"font-size: 1.6em\">Data Manipulation and Analysis</span>**\n",
    "\n",
    "- **pandas**: For data manipulation and analysis.\n",
    "- **numpy**: For numerical operations.\n",
    "\n",
    "**<span style=\"font-size: 1.6em\">Data Visualization</span>**\n",
    "\n",
    "- **matplotlib**: For creating static, interactive, and animated visualizations.\n",
    "- **seaborn**: For statistical data visualization, built on top of matplotlib.\n",
    "- **plotly**: For interactive visualizations.\n",
    "\n",
    "**<span style=\"font-size: 1.6em\">Statistical Analysis</span>**\n",
    "\n",
    "- **scipy**: For statistical tests and scientific computing.\n",
    "- **statsmodels**: For statistical modeling and hypothesis testing.\n",
    "\n",
    "**<span style=\"font-size: 1.6em\">Machine Learning</span>**\n",
    "\n",
    "- **scikit-learn**: For machine learning algorithms and tools.\n",
    "\n",
    "**<span style=\"font-size: 1.6em\">Additional Libraries</span>**\n",
    "\n",
    "- **yfinance**: For fetching historical market data from Yahoo Finance\n",
    "- **requests**: For making HTTP requests to download data\n",
    "- **datetime**: For manipulating dates and times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Manipulation and Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# Statistical Analysis\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Additional Libraries\n",
    "import yfinance as yf\n",
    "import requests\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"font-size: 1.6em\">Exploratory Data Analysis</span>**\n",
    "\n",
    "**Data Preprocessing**\n",
    "\n",
    "Data preprocessing is a crucial step in any data analysis project. It involves transforming raw data into a format that is suitable for analysis. This includes importing data, parsing and converting data types, handling missing values, and organizing the data into a structured format such as a pandas DataFrame.\n",
    "\n",
    "**Importing and Parsing Data**\n",
    "\n",
    "The first step in data preprocessing is to import the dataset into your Python environment and parse any necessary columns. This involves loading the data from a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Symbol     Security             GICS Sector               GICS Sub-Industry  \\\n",
      "0    MMM           3M             Industrials        Industrial Conglomerates   \n",
      "1    AOS  A. O. Smith             Industrials               Building Products   \n",
      "2    ABT       Abbott             Health Care           Health Care Equipment   \n",
      "3   ABBV       AbbVie             Health Care                   Biotechnology   \n",
      "4    ACN    Accenture  Information Technology  IT Consulting & Other Services   \n",
      "\n",
      "     Headquarters Location  Date added        CIK      Founded Exchange  \\\n",
      "0    Saint Paul, Minnesota  1957-03-04    66740.0         1902      NaN   \n",
      "1     Milwaukee, Wisconsin  2017-07-26    91142.0         1916      NaN   \n",
      "2  North Chicago, Illinois  1957-03-04     1800.0         1888      NaN   \n",
      "3  North Chicago, Illinois  2012-12-31  1551152.0  2013 (1888)      NaN   \n",
      "4          Dublin, Ireland  2011-07-06  1467373.0         1989      NaN   \n",
      "\n",
      "  Shortname  ... Fulltimeemployees Longbusinesssummary Weight  date  open  \\\n",
      "0       NaN  ...               NaN                 NaN    NaN   NaN   NaN   \n",
      "1       NaN  ...               NaN                 NaN    NaN   NaN   NaN   \n",
      "2       NaN  ...               NaN                 NaN    NaN   NaN   NaN   \n",
      "3       NaN  ...               NaN                 NaN    NaN   NaN   NaN   \n",
      "4       NaN  ...               NaN                 NaN    NaN   NaN   NaN   \n",
      "\n",
      "   high  low close volume Name  \n",
      "0   NaN  NaN   NaN    NaN  NaN  \n",
      "1   NaN  NaN   NaN    NaN  NaN  \n",
      "2   NaN  NaN   NaN    NaN  NaN  \n",
      "3   NaN  NaN   NaN    NaN  NaN  \n",
      "4   NaN  NaN   NaN    NaN  NaN  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "directory_path = '/Users/danielberhane/Desktop/CMSC320/CMSC320 Final_Project/CMSC320-Final_Group_project'\n",
    "\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Parse necessary columns \n",
    "        if 'Date' in df.columns:\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Display the first few rows to understand the structure of the combined data\n",
    "print(combined_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "**Cklean the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ty/czty13fj59zfllmbdt0yjqtw0000gn/T/ipykernel_78280/2778411921.py:5: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  combined_data['date'] = pd.to_datetime(combined_data['date'])\n"
     ]
    },
    {
     "ename": "DateParseError",
     "evalue": "Unknown datetime string format, unable to parse: Unknown, at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDateParseError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m     combined_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(combined_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m combined_data\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m----> 5\u001b[0m     combined_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     combined_data\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Handle missing values for numeric columns\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:1063\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1061\u001b[0m             result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[0;32m-> 1063\u001b[0m     cache_array \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_listlike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m   1065\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:247\u001b[0m, in \u001b[0;36m_maybe_cache\u001b[0;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[1;32m    245\u001b[0m unique_dates \u001b[38;5;241m=\u001b[39m unique(arg)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dates) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(arg):\n\u001b[0;32m--> 247\u001b[0m     cache_dates \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# GH#45319\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:435\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_strptime_with_fallback(arg, name, utc, \u001b[38;5;28mformat\u001b[39m, exact, errors)\n\u001b[0;32m--> 435\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m \u001b[43mobjects_to_datetime64\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n\u001b[1;32m    447\u001b[0m     out_unit \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdatetime_data(result\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/arrays/datetimes.py:2398\u001b[0m, in \u001b[0;36mobjects_to_datetime64\u001b[0;34m(data, dayfirst, yearfirst, utc, errors, allow_object, out_unit)\u001b[0m\n\u001b[1;32m   2395\u001b[0m \u001b[38;5;66;03m# if str-dtype, convert\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(data, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mobject_)\n\u001b[0;32m-> 2398\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m \u001b[43mtslib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray_to_datetime\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2400\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2403\u001b[0m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreso\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mabbrev_to_npy_unit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_unit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2405\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2408\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[1;32m   2409\u001b[0m     \u001b[38;5;66;03m#  is in UTC\u001b[39;00m\n\u001b[1;32m   2410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result, tz_parsed\n",
      "File \u001b[0;32mtslib.pyx:414\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtslib.pyx:596\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtslib.pyx:553\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mconversion.pyx:641\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.conversion.convert_str_to_tsobject\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsing.pyx:336\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsing.pyx:666\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.dateutil_parse\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDateParseError\u001b[0m: Unknown datetime string format, unable to parse: Unknown, at position 0"
     ]
    }
   ],
   "source": [
    "# Parse necessary columns \n",
    "if 'Date' in combined_data.columns:\n",
    "    combined_data['Date'] = pd.to_datetime(combined_data['Date'])\n",
    "elif 'date' in combined_data.columns:\n",
    "    combined_data['date'] = pd.to_datetime(combined_data['date'])\n",
    "    combined_data.rename(columns={'date': 'Date'}, inplace=True)\n",
    "\n",
    "# Handle missing values numeric\n",
    "numeric_columns = combined_data.select_dtypes(include=[np.number]).columns\n",
    "combined_data[numeric_columns] = combined_data[numeric_columns].fillna(combined_data[numeric_columns].mean())\n",
    "\n",
    "# Handle missing values  non-numeric columns\n",
    "non_numeric_columns = combined_data.select_dtypes(exclude=[np.number]).columns\n",
    "combined_data[non_numeric_columns] = combined_data[non_numeric_columns].fillna('Unknown')\n",
    "\n",
    "# Display summary verify the changes\n",
    "print(combined_data.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "***Organizing data ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Symbol     Security             GICS Sector               GICS Sub-Industry  \\\n",
      "0    MMM           3M             Industrials        Industrial Conglomerates   \n",
      "1    AOS  A. O. Smith             Industrials               Building Products   \n",
      "2    ABT       Abbott             Health Care           Health Care Equipment   \n",
      "3   ABBV       AbbVie             Health Care                   Biotechnology   \n",
      "4    ACN    Accenture  Information Technology  IT Consulting & Other Services   \n",
      "\n",
      "     Headquarters Location  Date added        CIK      Founded Exchange  \\\n",
      "0    Saint Paul, Minnesota  1957-03-04    66740.0         1902  Unknown   \n",
      "1     Milwaukee, Wisconsin  2017-07-26    91142.0         1916  Unknown   \n",
      "2  North Chicago, Illinois  1957-03-04     1800.0         1888  Unknown   \n",
      "3  North Chicago, Illinois  2012-12-31  1551152.0  2013 (1888)  Unknown   \n",
      "4          Dublin, Ireland  2011-07-06  1467373.0         1989  Unknown   \n",
      "\n",
      "  Shortname  ... Fulltimeemployees Longbusinesssummary    Weight     date  \\\n",
      "0   Unknown  ...          58195.42             Unknown  0.001988  Unknown   \n",
      "1   Unknown  ...          58195.42             Unknown  0.001988  Unknown   \n",
      "2   Unknown  ...          58195.42             Unknown  0.001988  Unknown   \n",
      "3   Unknown  ...          58195.42             Unknown  0.001988  Unknown   \n",
      "4   Unknown  ...          58195.42             Unknown  0.001988  Unknown   \n",
      "\n",
      "        open       high        low      close        volume     Name  \n",
      "0  83.023334  83.778311  82.256096  83.043763  4.321823e+06  Unknown  \n",
      "1  83.023334  83.778311  82.256096  83.043763  4.321823e+06  Unknown  \n",
      "2  83.023334  83.778311  82.256096  83.043763  4.321823e+06  Unknown  \n",
      "3  83.023334  83.778311  82.256096  83.043763  4.321823e+06  Unknown  \n",
      "4  83.023334  83.778311  82.256096  83.043763  4.321823e+06  Unknown  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Closing_Price'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Closing_Price'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example: Create a new column for 7-day moving average of the closing price\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m combined_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m7_day_MA\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mClosing_Price\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mrolling(window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Verify the new feature\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(combined_data\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Closing_Price'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
